[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "My Portfolio",
    "section": "",
    "text": "TidyTuesday Week 42\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouthern Resident Killer Whale Encounters\n\n\n\n\n\n\nr\n\n\ncode\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nAngie Guillory\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Data\n\n\n\n\n\n\nr\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 7, 2024\n\n\nAngie Guillory\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nAngie Guillory\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume",
    "section": "",
    "text": "Projects mentioned in the Resume: Analyzing Listeria Outbreaks and Texas C-Section Trends: What Age, Race, and Prenatal Visits Have to Do with It"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angie Guillory",
    "section": "",
    "text": "Hi, I‚Äôm Angie\nEspresso enthusiast, avid bookworm, and Ted Lasso binge-watcher, all while working out of üåçTexas, USA.\nI am an experienced HEDIS Coordinator with a robust public health and epidemiology background. With over three years of experience and a Master‚Äôs in Public Health."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Analysis of Data",
    "section": "",
    "text": "This was a project using data. You can find the project here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Tidy-Tuesday-Week-42/index.html",
    "href": "posts/Tidy-Tuesday-Week-42/index.html",
    "title": "Southern Resident Killer Whale Encounters",
    "section": "",
    "text": "I‚Äôm excited to submit my first #TidyTuesday project! Week 42 focuses on killer whales aka Orcas üêãI have to admit i‚Äôm a complete noob when it comes to whale or sea life knowledge at that, but this dataset is a great way to stretch my tidying data skills with R.\nJoin the #datafam and find out what #TidyTuesday is all about."
  },
  {
    "objectID": "posts/Tidy-Tuesday-Week-42/index.html#week-42-goods",
    "href": "posts/Tidy-Tuesday-Week-42/index.html#week-42-goods",
    "title": "Southern Resident Killer Whale Encounters",
    "section": "Week 42 Goods:",
    "text": "Week 42 Goods:\nThis week‚Äôs data comes from the Center for Whale Research CWR. Done as a web scraping side project- Jadey Ryan gathered encounter data from the CWR website. In 2023 she presented her methodology at a Seattle R-Ladies meetup.\nLink to dataset: https://github.com/rfordatascience/tidytuesday/blob/master/data/2024/2024-10-15/readme.md\nQuestions I‚Äôll be attempting to answer-\n\nWhich pods or ecotypes have the longest duration encounters with CWR researchers?\nAre there trends in where orca encounters occur over time?\n\nRead in and view the dataset:\n\n#Read in the data directly from GitHub\norcas &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-10-15/orcas.csv')\n\nRows: 775 Columns: 19\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (10): encounter_sequence, duration, vessel, observers, pods_or_ecotype,...\ndbl   (6): year, encounter_number, begin_latitude, begin_longitude, end_lati...\ndate  (1): date\ntime  (2): begin_time, end_time\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#view the dataset\nView(orcas)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Fun+Stuff",
    "section": "",
    "text": "Better Care, Smarter Spending: Exploring Provider Performance and Claim Trends\n\n\n\nR\n\n\n\n\n\n\n\nAngie Guillory\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBehind the Shots: Exploring Vaccine Adverse Events with Data\n\n\n\nExcel\n\n\nDashboard\n\n\nPowerBI\n\n\n\n\n\n\n\nAngie Guillory\n\n\nNov 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Listeria Outbreaks\n\n\n\nSAS\n\n\n\n\n\n\n\nAngie Guillory\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTexas C-Section Trends: What Age, Race, and Prenatal Visits Have to Do with It\n\n\n\nExcel\n\n\nSAS\n\n\n\n\n\n\n\nAngie Guillory\n\n\nNov 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTexas Stroke Hospitalization Trends\n\n\n\nExcel\n\n\nDashboard\n\n\n\n\n\n\n\nAngie Guillory\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Listeria/index.html",
    "href": "posts/Listeria/index.html",
    "title": "Analyzing Listeria Outbreaks",
    "section": "",
    "text": "Following the February 2024 investigation into Listeria monocytogenes contamination linked to Queso Fresco and Cotija cheeseüßÄ, I embarked on a comprehensive analysis of Listeria outbreaks over a 12-year period, focusing on the key trends and environments that drive these outbreaks.\nListeria monocytogenes (or L. monocytogenes) is a type of bacteria that can make you sick. It hangs out in places like soil, water, decaying plants, animals, and damp areas.\nYou usually pick up L. monocytogenes when food is handled or stored in places where the bacteria is present, like during harvesting, processing, or transporting. It can spread through things like raw materials, water, soil, or even the air. source\nPets can also bring the bacteria into your home if they eat contaminated food.\nWhat makes it tricky is that it can survive and even grow in the fridge, so it‚Äôs not always easy to get rid of. If you eat food contaminated with this bacteria, you might develop an illness called listeriosis.\nImage Source: FDA.gov\nAs part of my hands-on learning experience at Broadstreet Institute, I conducted an in-depth study of Listeria outbreak-related fatalities from 2009 to 2021. My project sought to uncover trends and pinpoint the environments most associated with Listeria monocytogenes.\nTo achieve these insights, I extensively utilized SAS programming for data cleaning, analysis, and visualization, transforming raw data into meaningful patterns."
  },
  {
    "objectID": "posts/Listeria/index.html#steps-for-analysis",
    "href": "posts/Listeria/index.html#steps-for-analysis",
    "title": "Analyzing Listeria Outbreaks",
    "section": "Steps for Analysis",
    "text": "Steps for Analysis\nStep 1 assumes the library reference is being created.\n\nStep 2: Import the National Outbreak Public Data Tool worksheet into SAS and check out the contents of the file;\nPROC IMPORT DATAFILE=NationalOutbreakPublicDataTool DBMS=XLSX OUT=WORK.IMPORT;\n    GETNAMES=YES;\nRUN;\n\nPROC CONTENTS DATA=WORK.IMPORT;\nRUN;\n\n\nStep 3: Rename the dataset and save it to the work library to clean and manipulate the data without losing the original info;\ndata listeria; set Import; RUN;\n\n\nStep 4: List the different settings where the events have occured, and list them in desc (greatest to least) order.;\nproc freq data=WORK.Listeria order=freq;\n    tables Setting / plots=(freqplot);\nrun;\n\n\nStep 5: Recategorize the different settings to reduce overlapping and over granulation of categories.;\nproc sql;\n    create table NewListeria as\n    select *,\n        case \n            when Setting like \"Private%\" then \"Private_Residence\"\n            when Setting like \"Restaurant%\" then \"Restaurant\"\n            when Setting like \"Long-term%\" or Setting like \"Hospital%\" then \"Healthcare_Facility\"\n            when Setting like \"Grocer%\" then \"Grocery_Store\"\n            when Setting like \"Banque%\" then \"Banquet_Facility\"\n            else Setting\n        end as New_Setting\n    from work.Listeria;\nquit;\n\n\nStep 6: Relist the newly categorized settings;\nproc freq data=WORK.NewListeria order=freq;\n    tables New_Setting / plots=(freqplot);\nrun;\n\n\nStep 7: Create frequency tables for Illnesses, Hospitalization and Deaths. Add a title and subtitle to each table. Remove cumulative columns;\nODS noproctitle;\nTitle1 \"Outbreak Settings Associated with Highest Number of Illnesses\";\ntitle2 \"Unknown and Other settings excluded\";\nproc freq data= NewListeria order=freq;\n    table New_Setting/nocum;\n    weight illnesses;\n    where New_Setting &lt;&gt; \"Other\" and New_Setting &lt;&gt; \"Unknown\";\nRUN;\n\n\nODS noproctitle;\nTitle1 \"Outbreak Settings Associated with Highest Number of Hospitalizations\";\ntitle2 \"Unknown and Other settings excluded\";\nproc freq data= NewListeria order=freq;\n    table New_Setting/nocum;\n    weight hospitalizations;\n    where New_Setting &lt;&gt; \"Other\" and New_Setting &lt;&gt; \"Unknown\";\nRUN;\n\n\nODS noproctitle;\nTitle1 \"Outbreak Settings Associated with Highest Number of Deaths\";\ntitle2 \"Unknown and Other settings excluded\";\nproc freq data= NewListeria order=freq;\n    table New_Setting/nocum;\n    weight deaths;\n    where New_Setting &lt;&gt; \"Other\" and New_Setting &lt;&gt; \"Unknown\";\nRUN;\nODS reset;\nTitle;\n\n\nStep 8: Create a bar chart that visualizes the frequency of Listeria outbreaks by year;\nods graphics / reset width=6.4in height=4.8in imagemap;\nproc sgplot data=WORK.NewListeria;\n    title height=14pt \"Frequency of Listeria Outbreaks by Year\";\n    vbar Year / fillattrs=(color=CX9a51d5) datalabel;\n    yaxis grid;\nrun;\nods graphics / reset;\ntitle;\n\n\n\n\n\n\nGithub Repository"
  },
  {
    "objectID": "posts/Listeria/index.html#objective",
    "href": "posts/Listeria/index.html#objective",
    "title": "Analyzing Listeria Outbreaks",
    "section": "Objective",
    "text": "Objective\nThe project aimed to:\n\nIdentify high-risk settings: Determine which environments (e.g., food processing facilities, retail outlets, or homes) were most frequently linked to illnesses, hospitalizations, and deaths.\nSpot significant trends: Assess whether any particular year showed a surge in outbreak frequency or severity."
  },
  {
    "objectID": "posts/Listeria/index.html#recommendations",
    "href": "posts/Listeria/index.html#recommendations",
    "title": "Analyzing Listeria Outbreaks",
    "section": "Recommendations",
    "text": "Recommendations\nAccording to the data, the most significant number of illnesses, hospitalizations, and deaths were linked to homes, and the maximum frequency of Listeria outbreaks from 2009 to 2021 occurred in 2014. These observations can direct public health initiatives and regulations to lessen the effects of Listeria outbreaks.\nBut why is the setting for exposure highest among private homes or residences?"
  },
  {
    "objectID": "posts/Stroke-Trends/index.html",
    "href": "posts/Stroke-Trends/index.html",
    "title": "Texas Stroke Hospitalization Trends",
    "section": "",
    "text": "Have you ever thought about how stroke hospitalizations might differ across Texas counties? It‚Äôs kinda wild when you break it down between hemorrhagic (the bleeding kind) and ischemic (the blockage kind) strokes.\nTo really examine this, you‚Äôve got to dig into the data to spot regional trends, risk factors, and even how healthcare resources are distributed. This can tell you a lot about why some areas might have different outcomes and what healthcare planners should keep in mind."
  },
  {
    "objectID": "posts/Stroke-Trends/index.html#this-project-used",
    "href": "posts/Stroke-Trends/index.html#this-project-used",
    "title": "Texas Stroke Hospitalization Trends",
    "section": "This project used:",
    "text": "This project used:\n\npower query\ncreating a combo chart\nslicers\ndynamic text boxes\n\nFor this, I grabbed some data on stroke hospitalizations in Texas counties. I used the Interactive Atlas of Heart Disease and Stroke to pull ‚Äúall stroke‚Äù data for 2018-2020, focusing on folks 65 and older. I applied filters for all racial and ethnic groups, and I ran them separately for hemorrhagic and ischemic strokes.\nThen, I opened Excel and copied the data from the site straight into a spreadsheet. I set up separate worksheets for all strokes, ischemic strokes, and hemorrhagic strokes. After that, I used Excel‚Äôs Power Query to combine all the data tables into one. I named it ‚Äúcombinedtable,‚Äù closed it, and loaded it into a new worksheet to keep things tidy.\nTo make things visually pop, I highlighted the state of Texas on the map, drawing attention to the fact that the average stroke hospitalization rate among Medicare beneficiaries nationwide is 10.7 per 1,000. At the same time, Texas sits slightly higher at 11.8 per 1,000. The combo chart I made showed that nearly half (45%) of all counties in Texas have stroke hospitalization rates above the state average for Medicare beneficiaries. I also added a slicer so you can choose a specific county and see how it affects the chart.\n\n\n\n\n\n\nAt the bottom of the dashboard, there‚Äôs a text box with a key stat: 87% of reported strokes were ischemic (2,187 cases), and the other 13% were hemorrhagic (341 cases).\n\n\n\nIn conclusion, these visualizations and data help paint a clear picture of how stroke hospitalizations vary across Texas. Understanding these differences can guide more targeted healthcare interventions and help allocate resources where needed most. It‚Äôs fascinating to see how much insight data like this can provide!"
  },
  {
    "objectID": "posts/Stroke-Trends/index.html#this-excel-project-used",
    "href": "posts/Stroke-Trends/index.html#this-excel-project-used",
    "title": "Texas Stroke Hospitalization Trends",
    "section": "This Excel project used:",
    "text": "This Excel project used:\n\npower query\ncreating a combo chart\nslicers\ndynamic text boxes\n\nFor this, I grabbed some data on stroke hospitalizations in Texas counties. I used the Interactive Atlas of Heart Disease and Stroke to pull ‚Äúall stroke‚Äù data for 2018-2020, focusing on folks 65 and older. I applied filters for all racial and ethnic groups, and I ran them separately for hemorrhagic and ischemic strokes.\n\n\n\nMapping Tool, source: https://nccd.cdc.gov/\n\n\nThen, I opened Excel and copied the data from the site straight into a spreadsheet. I set up separate worksheets for all strokes, ischemic strokes, and hemorrhagic strokes. After that, I used Excel‚Äôs Power Query to combine all the data tables into one. I named it ‚Äúcombinedtable,‚Äù closed it, and loaded it into a new worksheet to keep things tidy.\nTo make things visually pop, I highlighted the state of Texas on the map, drawing attention to the fact that the average stroke hospitalization rate among Medicare beneficiaries nationwide is 10.7 per 1,000. At the same time, Texas sits slightly higher at 11.8 per 1,000. The combo chart I made showed that nearly half (45%) of all counties in Texas have stroke hospitalization rates above the state average for Medicare beneficiaries. I also added a slicer so you can choose a specific county and see how it affects the chart.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the bottom of the dashboard, there‚Äôs a text box with a key stat: 87% of reported strokes were ischemic (2,187 cases), and the other 13% were hemorrhagic (341 cases).\n\n\n\nIn conclusion, these visualizations and data help paint a clear picture of how stroke hospitalizations vary across Texas. Understanding these differences can guide more targeted healthcare interventions and help allocate resources where needed most. It‚Äôs fascinating to see how much insight data like this can provide!\n\n\n\n\n\n\nGithub Repository"
  },
  {
    "objectID": "posts/MLS-in-SAS/index.html",
    "href": "posts/MLS-in-SAS/index.html",
    "title": "Texas C-Section Trends: What Age, Race, and Prenatal Visits Have to Do with It",
    "section": "",
    "text": "I investigated the relationship between the age and race of mothers, the number of prenatal visits and the average number of cesarean deliveries in Texas for 2022. Specifically, how the independent variables‚ÄîAge of Mother, Prenatal Visits, and the Race of the Mother‚Äîaffect the dependent variable, cesarean birth rate."
  },
  {
    "objectID": "posts/MLS-in-SAS/index.html#this-excel-project-used",
    "href": "posts/MLS-in-SAS/index.html#this-excel-project-used",
    "title": "Multilinear Regression Modeling in SAS",
    "section": "This Excel project used:",
    "text": "This Excel project used:\n\npower query\ncreating a combo chart\nslicers\ndynamic text boxes\n\nFor this, I grabbed some data on stroke hospitalizations in Texas counties. I used the Interactive Atlas of Heart Disease and Stroke to pull ‚Äúall stroke‚Äù data for 2018-2020, focusing on folks 65 and older. I applied filters for all racial and ethnic groups, and I ran them separately for hemorrhagic and ischemic strokes.\n\n\n\nMapping Tool, source: https://nccd.cdc.gov/\n\n\nThen, I opened Excel and copied the data from the site straight into a spreadsheet. I set up separate worksheets for all strokes, ischemic strokes, and hemorrhagic strokes. After that, I used Excel‚Äôs Power Query to combine all the data tables into one. I named it ‚Äúcombinedtable,‚Äù closed it, and loaded it into a new worksheet to keep things tidy.\nTo make things visually pop, I highlighted the state of Texas on the map, drawing attention to the fact that the average stroke hospitalization rate among Medicare beneficiaries nationwide is 10.7 per 1,000. At the same time, Texas sits slightly higher at 11.8 per 1,000. The combo chart I made showed that nearly half (45%) of all counties in Texas have stroke hospitalization rates above the state average for Medicare beneficiaries. I also added a slicer so you can choose a specific county and see how it affects the chart.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the bottom of the dashboard, there‚Äôs a text box with a key stat: 87% of reported strokes were ischemic (2,187 cases), and the other 13% were hemorrhagic (341 cases).\n\n\n\nIn conclusion, these visualizations and data help paint a clear picture of how stroke hospitalizations vary across Texas. Understanding these differences can guide more targeted healthcare interventions and help allocate resources where needed most. It‚Äôs fascinating to see how much insight data like this can provide!\n\n\n\n\n\n\nGithub Repository"
  },
  {
    "objectID": "posts/MLS-in-SAS/index.html#background-info",
    "href": "posts/MLS-in-SAS/index.html#background-info",
    "title": "Texas C-Section Trends: What Age, Race, and Prenatal Visits Have to Do with It",
    "section": "Background Info:",
    "text": "Background Info:\nIn 2021, around a third of US births were C-sections, a surgical procedure even for low-risk cases. Unnecessary C-sections can harm mothers and babies more than natural births. Overuse is due to differing medical practices and a casual approach to surgery.\nThe American College of Obstetricians and Gynecologists found slow labor progression to be a common reason for first-time C-sections. Encouraging natural births for low-risk women could reduce these unnecessary procedures, improving health outcomes."
  },
  {
    "objectID": "posts/MLS-in-SAS/index.html#data-source",
    "href": "posts/MLS-in-SAS/index.html#data-source",
    "title": "Texas C-Section Trends: What Age, Race, and Prenatal Visits Have to Do with It",
    "section": "Data Source:",
    "text": "Data Source:\nFor this, I grabbed some data on cesarean deliveries from the CDC WONDER online database.\nQuery Parameters included:\n\nFinal Route and Delivery Method: Cesarean\nNumber of Previous Cesareans: 0 previous cesareans; 1 previous cesareans; 2 previous cesareans; 3 previous cesareans; 4 or more previous cesareans\nStates: Texas (48)\nYear: 2022\nGroup By: County of Residence; Mother‚Äôs Single Race 6; Age of Mother 9; Number of Prenatal Visits\nShow Totals: Disabled\nShow Zero Values: Disabled\nShow Suppressed: False\nCalculate Rates Per: 1,000\n\n\nStep 3: Examine the data and check the distribution\n\nThe Means Procedure\nproc means data= work.cdelivery mean var range stddev min q1 median q3 max maxdec= 2;\n    var birth_rate visits_code; *numeric variables;\nrun;\n\n\n\n^^the means procedure Output^^\n\n\nI examined the data by checking the distribution of key variables, and found the following:\n\nOn average, the cesarean birth rate is 47.11 per 1,000 live births.\nThe large variance (5159) indicates the birth rate values are spread out over a wide range.\nThis observation carries over to the standard deviation Std Dev. As the Std Dev is greater than the mean (71.83&gt;47.11) this tells me that the data is further away from the mean, is highly skewed, and has outliers.\n\n\n\n\n\n\n\nThe Std Dev is calculated as the square root of the variance.\n\n\n\nThe distribution of the data is most likely positive or right-skewed with a long tail towards higher values. This is seen as the mean (47.11) is to the left of the maximum value in the dataset (936.00).\n\nMothers had an average of 13 (12.89) prenatal visits throughout their pregnancy.\nCompared to the cesarean birth rate, the variance for prenatal visits (268.55) is less spread out in the dataset.\nAs seen before with the birth rate, the Std Dev for prenatal visits is higher compared to the mean (16.39&gt;12.89).\n\nThe distribution of prenatal visits appears to be moderately spread, with a Std Dev (16.39) greater than the mean (12.89). The maximum value (99.00) suggests some outliers, but the majority of observations are found around the mean (12.89).\n\n\nThe Freq Procedure\nproc freq data= work.cdelivery;\n    tables age_of_mother mother_race/ plots=(freqplot); *categorical variables;\nrun;\n \n\nWhite mothers are the majority group, representing 72.12% of the total population, followed by Black or African American mothers at 19.12%.\nThe largest age group is 30-34 years, with 27.34% of the mothers, followed closely by 25-29 years at 24.87%."
  },
  {
    "objectID": "posts/MLS-in-SAS/index.html#the-means-procedure",
    "href": "posts/MLS-in-SAS/index.html#the-means-procedure",
    "title": "Multilinear Regression Modeling in SAS",
    "section": "The Means Procedure",
    "text": "The Means Procedure\nproc means data= work.cdelivery mean var range stddev min q1 median q3 max maxdec= 2;\n    var birth_rate visits_code; *numeric variables;\nrun;\n\n\n\n^^Step 3 Output^^\n\n\nI examined the data by checking the distribution of key variables, and found the following:\n\nOn average, the birth rate is 47.11 per 1,000 live births.\nThe large variance (5159) indicates the birth rate values are spread out over a wide range.\nThis observation carries over to the standard deviation Std Dev. As the Std Dev is greater than the mean (71.83&gt;47.11) this tells me that the data is further away from the mean, is highly skewed, and has outliers.\n\n\n\n\n\n\n\nThe Std Dev is calculated as the square root of the variance.\n\n\n\nThe distribution of the data is most likely positive or right-skewed with a long tail towards higher values. This is seen as the mean (47.11) is to the left of the maximum value in the dataset (936.00).\n\nMothers had an average of 13 (12.89) prenatal visits throughout their pregnancy.\nCompared to the cesarean birth rate, the variance for prenatal visits (268.55) is less spread out in the dataset.\nAs seen before with the birth rate, the Std Dev for prenatal visits is higher compared ot the mean (16.39&gt;12.89).\n\nThe distribution of prenatal visits appears to be moderately spread, with a Std Dev (16.39) greater than the mean (12.89). The maximum value (99.00) suggests some outliers, but the majority of observations are found around the mean (12.89)."
  },
  {
    "objectID": "posts/MLS-in-SAS/index.html#the-freq-procedure",
    "href": "posts/MLS-in-SAS/index.html#the-freq-procedure",
    "title": "Multilinear Regression Modeling in SAS",
    "section": "The Freq Procedure",
    "text": "The Freq Procedure\nproc freq data= work.cdelivery;\n    tables age_of_mother mother_race/ plots=(freqplot); *categorical variables;\nrun;"
  },
  {
    "objectID": "posts/MLS-in-SAS/index.html#step-4-use-proc-glm-to-model-birth-rate-as-a-continuous-dependent-variable-and-include-the-independent-variables-age-of-mother-visits-code-and-mother-rate",
    "href": "posts/MLS-in-SAS/index.html#step-4-use-proc-glm-to-model-birth-rate-as-a-continuous-dependent-variable-and-include-the-independent-variables-age-of-mother-visits-code-and-mother-rate",
    "title": "Texas C-Section Trends: What Age, Race, and Prenatal Visits Have to Do with It",
    "section": "Step 4: Use Proc GLM to model Birth Rate as a continuous dependent variable, and include the independent variables (Age of Mother, Visits Code, and Mother Rate)",
    "text": "Step 4: Use Proc GLM to model Birth Rate as a continuous dependent variable, and include the independent variables (Age of Mother, Visits Code, and Mother Rate)\nproc glm data=cdelivery;\n   class Mother_Race Age_of_Mother;     *categorical variables;\n   model Birth_Rate = Age_of_Mother Visits_Code Mother_Race;   *Main effects model;\n   means Age_of_Mother / tukey;    *Post-hoc tests to check significant differences \n                                    between age groups;\nrun;\n\n\n\n^^GLM procedure Output^^\n\n\n\nSo what does the model tell us about observing the age of mother, the mother‚Äôs race and the number of prenatal visits for predicting cesarean birth rates?\nThe model explains less than 5% of the variation in cesarean birth rates, according to its R-Square of 4.47%.The difference in birth rates is not well supported by the factors I selected from the dataset: the mother‚Äôs age, race, and number of prenatal visits.\nWith a p-value &lt;.0001, the model is statistically significant overall, indicating that these predictors collectively have a meaningful relationship with the birth rate. However, the low R-squared value suggests that the majority of the variance in the birth rate cannot be explained.\nThe age of the mother has a significant impact on birth rate (F = 8.89, p &lt; .0001), meaning birth rates likely vary across different age groups. Similarly, mother‚Äôs race also significantly affects birth rate (F = 21.96, p &lt; .0001), suggesting birth rates differ among racial groups. However, the number of prenatal visits doesn‚Äôt appear to be a meaningful predictor here (p = 0.9957), as it doesn‚Äôt show any significant influence on birth rate."
  },
  {
    "objectID": "posts/VAERSdata/index.html",
    "href": "posts/VAERSdata/index.html",
    "title": "Behind the Shots: Exploring Vaccine Adverse Events with Data",
    "section": "",
    "text": "Ever wondered what vaccine adverse event data can tell us about safety trends and outcomes? Using the CDC WONDER database, I built a Power BI dashboard to break down years of vaccine-related reports into clear, actionable insights. Let me take you behind the scenes to show how it all came together‚Äîfrom raw data to polished visuals‚Äîand what the numbers are saying."
  },
  {
    "objectID": "posts/VAERSdata/index.html#this-excel-project-used",
    "href": "posts/VAERSdata/index.html#this-excel-project-used",
    "title": "Vaccine Adverse Events Dashboard 2019- 2024",
    "section": "This Excel project used:",
    "text": "This Excel project used:\n\npower query\ncreating a combo chart\nslicers\ndynamic text boxes\n\nFor this, I grabbed some data on stroke hospitalizations in Texas counties. I used the Interactive Atlas of Heart Disease and Stroke to pull ‚Äúall stroke‚Äù data for 2018-2020, focusing on folks 65 and older. I applied filters for all racial and ethnic groups, and I ran them separately for hemorrhagic and ischemic strokes.\n\n\n\nMapping Tool, source: https://nccd.cdc.gov/\n\n\nThen, I opened Excel and copied the data from the site straight into a spreadsheet. I set up separate worksheets for all strokes, ischemic strokes, and hemorrhagic strokes. After that, I used Excel‚Äôs Power Query to combine all the data tables into one. I named it ‚Äúcombinedtable,‚Äù closed it, and loaded it into a new worksheet to keep things tidy.\nTo make things visually pop, I highlighted the state of Texas on the map, drawing attention to the fact that the average stroke hospitalization rate among Medicare beneficiaries nationwide is 10.7 per 1,000. At the same time, Texas sits slightly higher at 11.8 per 1,000. The combo chart I made showed that nearly half (45%) of all counties in Texas have stroke hospitalization rates above the state average for Medicare beneficiaries. I also added a slicer so you can choose a specific county and see how it affects the chart.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the bottom of the dashboard, there‚Äôs a text box with a key stat: 87% of reported strokes were ischemic (2,187 cases), and the other 13% were hemorrhagic (341 cases).\n\n\n\nIn conclusion, these visualizations and data help paint a clear picture of how stroke hospitalizations vary across Texas. Understanding these differences can guide more targeted healthcare interventions and help allocate resources where needed most. It‚Äôs fascinating to see how much insight data like this can provide!\n\n\n\n\n\n\nGithub Repository"
  },
  {
    "objectID": "posts/VAERSdata/index.html#click-to-watch-the-video",
    "href": "posts/VAERSdata/index.html#click-to-watch-the-video",
    "title": "Vaccine Adverse Events Dashboard",
    "section": "Click to watch the video",
    "text": "Click to watch the video\n\n\n\n\n\n\nFigure¬†1: The video ‚ÄúVaccine Adverse Events PowerBi Dashboard‚Äù\n\n\n\nIn Figure¬†1‚Ä¶\nThis slides‚Äôs background video will play in a loop with audio muted.\nIt‚Äôs kinda wild when you break it down between hemorrhagic (the bleeding kind) and ischemic (the blockage kind) strokes.\nTo really examine this, you‚Äôve got to dig into the data to spot regional trends, risk factors, and even how healthcare resources are distributed. This can tell you a lot about why some areas might have different outcomes and what healthcare planners should keep in mind."
  },
  {
    "objectID": "posts/VAERSdata/index.html#vaccine-adverse-events-powerbi-dashboard",
    "href": "posts/VAERSdata/index.html#vaccine-adverse-events-powerbi-dashboard",
    "title": "Behind the Shots: Exploring Vaccine Adverse Events with Data",
    "section": "Vaccine Adverse Events PowerBi Dashboard",
    "text": "Vaccine Adverse Events PowerBi Dashboard\nPress play to watch the video.\n\n\nThe Big Picture\nThe goal of this project was simple: make sense of vaccine adverse event data from 2019 to 2024. I wanted to understand patterns across demographics, manufacturers, and event categories like hospitalizations, deaths, and emergency visits.\nTo do this, I built a Power BI dashboard that highlights:\n\nTop states with the highest number of reported adverse events.\nManufacturers with the most reports.\nYear-over-year trends and event categories.\n\nWhy? Because data-driven insights like these can help public health experts and vaccine makers improve safety and communication efforts."
  },
  {
    "objectID": "posts/VAERSdata/index.html#how-i-made-it-happen",
    "href": "posts/VAERSdata/index.html#how-i-made-it-happen",
    "title": "Behind the Shots: Exploring Vaccine Adverse Events with Data",
    "section": "How I Made It Happen",
    "text": "How I Made It Happen\n\n1. Starting with the Data\nI pulled data from the CDC WONDER database, which is an incredible (and free!) resource for public health info. Here‚Äôs how I tackled it:\n\nFirst, I filtered the dataset by gender‚Äîone for male reports and another for female reports.\nI downloaded the data as text files for each group.\n\n\n\n2. Prepping the Data\nNext, I turned to Excel‚Äôs Power Query tool to clean and combine the data:\n\nImported the male and female files into separate tables.\nAdded a column to each table to label gender.\nCreated another column to identify the country (spoiler: it‚Äôs all U.S. data).\nAppended the male and female tables into one final dataset, saving it as an Excel file for Power BI.\n\nThis gave me a clean, unified dataset ready to analyze.\n\n\n3. Building the Dashboard\nNow for the fun part‚Äîturning the data into visuals that make sense at a glance:\n\nMap Visualization: Showcased the top 10 states with the most adverse events.\nBar and Column Charts:\n\nOne chart displayed event counts by year, showing trends from 2019 to 2024.\nAnother broke down event counts by manufacturer.\n\nData Table: A detailed view of event categories and their counts.\n\nThe dashboard isn‚Äôt just pretty‚Äîit‚Äôs interactive. You can filter it by demographics, years, and event types to explore the story from different angles.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the Numbers Are Saying\nHere are the key takeaways:\n\nEmergency Care Dominates\nMost of the reports (513!) were about emergency room or office visits. These events are urgent but usually not life-threatening, making this category the biggest slice of the pie.\nMerck & Co., Inc.¬†Tops the Manufacturer List\nWith 157 adverse events reported, Merck leads the pack. This doesn‚Äôt necessarily mean their vaccines are riskier‚Äîit could be tied to how often their vaccines are used.\n2019 Stands Out\nOver half (52.57%) of all reports came from 2019. Was it a big vaccination push? A change in reporting practices? That‚Äôs worth digging into.\nCalifornia Leads the States\nAmong the top 10 states, California reported the highest number of events (37), followed by Florida (33) and Ohio (30).\n\n\n\nWhy This Matters\nProjects like this aren‚Äôt just about crunching numbers‚Äîthey‚Äôre about using data to make smarter decisions. Public health experts can use insights like these to:\n\nInvestigate trends in high-reporting states.\nDig deeper into manufacturers with more reports to identify opportunities for improvement.\nReassure the public that the vast majority of adverse events aren‚Äôt severe.\n\n\n\nThe Takeaway\nThis project was all about turning complex vaccine data into something that‚Äôs easy to understand and explore. Whether it‚Äôs identifying trends, pinpointing areas of concern, or just presenting the data in a user-friendly way, dashboards like this help bridge the gap between raw numbers and actionable insights.\n\n\n\n\n\n\nGithub Repository"
  },
  {
    "objectID": "posts/ClaimsData/index.html",
    "href": "posts/ClaimsData/index.html",
    "title": "Better Care, Smarter Spending: Exploring Provider Performance and Claim Trends",
    "section": "",
    "text": "Healthcare organizations are typically interested in gaining insights about optimizing patient care, reducing unnecessary expenses, provider performance and improving overall health outcomes."
  },
  {
    "objectID": "posts/ClaimsData/index.html#step-1-identify-the-goals-and-metrics",
    "href": "posts/ClaimsData/index.html#step-1-identify-the-goals-and-metrics",
    "title": "Better Care, Smarter Spending: Exploring Provider Performance and Claim Trends",
    "section": "Step 1 Identify the Goal(s) and Metrics",
    "text": "Step 1 Identify the Goal(s) and Metrics\nüîç Identify cost inefficiencies and specialty-specific trends that impact financial performance.\n\nMetric: Trends in claim amount by provider specialty. Is there a consistent rise or dip, or any unusual trends?\n\nüîç Identify patterns in claim denials by specialty, which could highlight issues in billing practices, claims processing, or insurance policies.\n\nMetric: Financial impact of denied claims by Specialty (total $ amount of denied claims/ total $ of claims for each specialty)\n\nüîç Identify which claim submission methods are most effective and which ones are associated with higher denial rates\n\nMetric: Trends in claims submissions to see which are more successful. Cost per claim submission method (paper, online [electronic] or phone) and claim status (approved, denied and pending)."
  },
  {
    "objectID": "posts/ClaimsData/index.html#step-2-collect-prepare-the-data",
    "href": "posts/ClaimsData/index.html#step-2-collect-prepare-the-data",
    "title": "Better Care, Smarter Spending: Exploring Provider Performance and Claim Trends",
    "section": "Step 2 Collect & Prepare the Data",
    "text": "Step 2 Collect & Prepare the Data\nWe‚Äôll be using Le‚ÄôAndre Nash‚Äôs incredible ‚ÄúEnhanced Health Insurance Claims Dataset‚Äù from Kaggle.\n\n\n\n\n\n\nImportant\n\n\n\nNote from the Creator: The data is entirely synthetic and generated using the Faker library.\n\n\nüìÇ Other data sources to consider adding:\n\nElectronic Health Records EHRs to look at clinic visits\nVariables like patient comorbidities and geographic location (e.g., urban vs.¬†rural)\nPatient socioeconomic status\n\nCommon issues in this step are missing data points or incomplete claims. Date formatting issues or duplicate records, and unusually high or low values (outliers). Let‚Äôs also check the range for the ClaimDate column.\n\nLet‚Äôs perform some checkpoints on the dataset.\n\n## Load the dataset into Rstudio environment\nlibrary(readr)\n\n## load data visualization and data manipulation packages\nlibrary(ggplot2)\nlibrary(dplyr)\n\nclaimsdata &lt;- read_csv(\"C:/Users/guilz/OneDrive/Home/R/data/enhanced_health_insurance_claims.csv\")\n\nhead(claimsdata)\n\n## Load tidyverse library\nlibrary(tidyverse)\n\nAre there missing datapoints in the dataset?\n\n## count of total missing values for the entire dataset\nsum(is.na(claimsdata))\n\n[1] 0\n\n## no missing data points\n\n\n## use the glimpse function to see the number of rows, columns and types of variables. \nglimpse(claimsdata)\n\nRows: 4,500\nColumns: 17\n$ ClaimID                 &lt;chr&gt; \"10944daf-f7d5-4e1d-8216-72ffa609fe41\", \"fcbeb‚Ä¶\n$ PatientID               &lt;chr&gt; \"8552381d-7960-4f64-b190-b20b8ada00a1\", \"327f4‚Ä¶\n$ ProviderID              &lt;chr&gt; \"4a4cb19c-4863-41cf-84b0-c2b21aace988\", \"422e0‚Ä¶\n$ ClaimAmount             &lt;dbl&gt; 3807.95, 9512.07, 7346.74, 6026.72, 1644.58, 1‚Ä¶\n$ ClaimDate               &lt;date&gt; 2024-06-07, 2023-05-30, 2022-09-27, 2023-06-2‚Ä¶\n$ DiagnosisCode           &lt;chr&gt; \"yy006\", \"tD052\", \"zx832\", \"kr421\", \"LZ261\", \"‚Ä¶\n$ ProcedureCode           &lt;chr&gt; \"hd662\", \"mH831\", \"dg637\", \"kG326\", \"cx805\", \"‚Ä¶\n$ PatientAge              &lt;dbl&gt; 16, 27, 40, 65, 24, 57, 40, 5, 74, 37, 5, 44, ‚Ä¶\n$ PatientGender           &lt;chr&gt; \"M\", \"M\", \"F\", \"M\", \"M\", \"M\", \"M\", \"M\", \"F\", \"‚Ä¶\n$ ProviderSpecialty       &lt;chr&gt; \"Cardiology\", \"Pediatrics\", \"Cardiology\", \"Neu‚Ä¶\n$ ClaimStatus             &lt;chr&gt; \"Pending\", \"Approved\", \"Pending\", \"Pending\", \"‚Ä¶\n$ PatientIncome           &lt;dbl&gt; 90279.43, 130448.02, 82417.54, 68516.96, 84122‚Ä¶\n$ PatientMaritalStatus    &lt;chr&gt; \"Married\", \"Single\", \"Divorced\", \"Widowed\", \"M‚Ä¶\n$ PatientEmploymentStatus &lt;chr&gt; \"Retired\", \"Student\", \"Employed\", \"Student\", \"‚Ä¶\n$ ProviderLocation        &lt;chr&gt; \"Jameshaven\", \"Beltrantown\", \"West Charlesport‚Ä¶\n$ ClaimType               &lt;chr&gt; \"Routine\", \"Routine\", \"Emergency\", \"Routine\", ‚Ä¶\n$ ClaimSubmissionMethod   &lt;chr&gt; \"Paper\", \"Online\", \"Online\", \"Phone\", \"Phone\",‚Ä¶\n\n## to get a list of data types (I prefer this method)\nmap(claimsdata, class)\n\n$ClaimID\n[1] \"character\"\n\n$PatientID\n[1] \"character\"\n\n$ProviderID\n[1] \"character\"\n\n$ClaimAmount\n[1] \"numeric\"\n\n$ClaimDate\n[1] \"Date\"\n\n$DiagnosisCode\n[1] \"character\"\n\n$ProcedureCode\n[1] \"character\"\n\n$PatientAge\n[1] \"numeric\"\n\n$PatientGender\n[1] \"character\"\n\n$ProviderSpecialty\n[1] \"character\"\n\n$ClaimStatus\n[1] \"character\"\n\n$PatientIncome\n[1] \"numeric\"\n\n$PatientMaritalStatus\n[1] \"character\"\n\n$PatientEmploymentStatus\n[1] \"character\"\n\n$ProviderLocation\n[1] \"character\"\n\n$ClaimType\n[1] \"character\"\n\n$ClaimSubmissionMethod\n[1] \"character\"\n\n\nAre there any duplicated ClaimIDs?\n\ngroup_by(claimsdata, ClaimID) %&gt;%\n filter(n() &gt;1)\n\n# A tibble: 0 √ó 17\n# Groups:   ClaimID [0]\n# ‚Ñπ 17 variables: ClaimID &lt;chr&gt;, PatientID &lt;chr&gt;, ProviderID &lt;chr&gt;,\n#   ClaimAmount &lt;dbl&gt;, ClaimDate &lt;date&gt;, DiagnosisCode &lt;chr&gt;,\n#   ProcedureCode &lt;chr&gt;, PatientAge &lt;dbl&gt;, PatientGender &lt;chr&gt;,\n#   ProviderSpecialty &lt;chr&gt;, ClaimStatus &lt;chr&gt;, PatientIncome &lt;dbl&gt;,\n#   PatientMaritalStatus &lt;chr&gt;, PatientEmploymentStatus &lt;chr&gt;,\n#   ProviderLocation &lt;chr&gt;, ClaimType &lt;chr&gt;, ClaimSubmissionMethod &lt;chr&gt;\n\n## no, there are no duplicated claims\n\nSince we are not going to be using the patient age or patient income columns, lets analyze the claim amount field for outliers using boxplots.\n\nggplot(claimsdata, aes(ProviderSpecialty, ClaimAmount, fill = ProviderSpecialty)) +\n  ggtitle(\"Boxplots\") +\n  xlab(\"Provider Specialty\") + ylab(\"Claim Amount in USD\") +\n  geom_boxplot() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) ## Rotate x-axis labels for readability\n\n\n\n\n\n\n\n\n\nprint(IQR(claimsdata$ClaimAmount)) ## measures the spread of the middle 50% of values\n\n[1] 4953.38\n\nprint(range(claimsdata$ClaimAmount)) ## min value and the max value\n\n[1]  100.12 9997.20\n\nprint(mean(claimsdata$ClaimAmount)) ## the average claim amount\n\n[1] 5014.204\n\n\nSince 50% of the data is ~$4953.38, let‚Äôs calculate the lower and upper bounds using the IQR. Then flag values that fall outside these bounds as potential outliers.\n\niqr &lt;- IQR(claimsdata$ClaimAmount)\n\n## Calculate Q1 and Q3\nq1 &lt;- quantile(claimsdata$ClaimAmount, 0.25) \nq3 &lt;- quantile(claimsdata$ClaimAmount, 0.75) \n\n## Calculate the lower and upper bounds\nlower_bound &lt;- q1 - 1.5 * iqr ## 25% of the data\nupper_bound &lt;- q3 + 1.5 * iqr ## 75% of the data\n\nprint(lower_bound)\n\n      25% \n-4920.997 \n\nprint(upper_bound)\n\n     75% \n14892.52 \n\n\nLet‚Äôs see if there are any outliers for the claims amount column\n\n## Identify the outliers\n\noutliers &lt;- claimsdata %&gt;% \n   filter(ClaimAmount &lt; lower_bound | ClaimAmount &gt; upper_bound)\n\nprint(outliers)\n\n# A tibble: 0 √ó 17\n# ‚Ñπ 17 variables: ClaimID &lt;chr&gt;, PatientID &lt;chr&gt;, ProviderID &lt;chr&gt;,\n#   ClaimAmount &lt;dbl&gt;, ClaimDate &lt;date&gt;, DiagnosisCode &lt;chr&gt;,\n#   ProcedureCode &lt;chr&gt;, PatientAge &lt;dbl&gt;, PatientGender &lt;chr&gt;,\n#   ProviderSpecialty &lt;chr&gt;, ClaimStatus &lt;chr&gt;, PatientIncome &lt;dbl&gt;,\n#   PatientMaritalStatus &lt;chr&gt;, PatientEmploymentStatus &lt;chr&gt;,\n#   ProviderLocation &lt;chr&gt;, ClaimType &lt;chr&gt;, ClaimSubmissionMethod &lt;chr&gt;\n\n\nThere are no outliers. Let‚Äôs also check the range for the ClaimDate column.\n\nprint(range(claimsdata$ClaimDate)) ## the dates cover 2 years\n\n[1] \"2022-07-09\" \"2024-07-08\"\n\n\n\n\nStep 3 Analyze the Data\nLet‚Äôs analyze the data and look at each key metric.\nTrends in claim amount by provider specialty. Is there a consistent rise or dip, or any unusual trends?\nHealthcare organizations can determine where costs are increasing unnecessarily or where cost-effective care is being provided by examining the average claim amounts over time for various provider specialties.\n\n## Aggregate the claimamount column to calculate the average claim amount per provider specialty\n\n## use the pipe operator to take the output and pass it on as the first argument into the function on the right.\n\nlibrary(lubridate)\n\ntrend &lt;- claimsdata %&gt;% \n  mutate(Year= year(ClaimDate)) %&gt;%       ## create the year column\n  group_by(ProviderSpecialty, Year) %&gt;%   ## group by specialty and year\n  summarize(AvgClaim = mean(ClaimAmount, na.rm = TRUE))  ## average claim amount by specialty and year, remove nulls\n\n`summarise()` has grouped output by 'ProviderSpecialty'. You can override using\nthe `.groups` argument.\n\nhead(trend)\n\n# A tibble: 6 √ó 3\n# Groups:   ProviderSpecialty [2]\n  ProviderSpecialty  Year AvgClaim\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n1 Cardiology         2022    4914.\n2 Cardiology         2023    5107.\n3 Cardiology         2024    4929.\n4 General Practice   2022    5028.\n5 General Practice   2023    4980.\n6 General Practice   2024    4941.\n\n## Create the line graph with all specialties\nggplot(trend, aes(x = Year, y = AvgClaim, color = ProviderSpecialty)) +\n  geom_line(linewidth = 1) + \n  labs(title = \"Trend of Average Claim Amount by Specialty\",\n       x = \"Year\", \n       y = \"Average Claim Amount in USD\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) ## Rotate x-axis labels for readability\n\n\n\n\n\n\n\n\nAll specialties have a downward trend except Pediatrics. The average claim amount for providers specializing in Pediatrics seems to be increasing from 2022 to 2024. I wonder if these results are expected?\nA consistent rise in claim amounts for Pediatrics could be due to factors like changes in the cost of pediatric procedures, new medical treatments, or increased demand for pediatric services. However, it could also raise concerns about potential issues like over-billing or unnecessary procedures, which would need further investigation.\n\n\nLet‚Äôs keep going onto our second metric.\nFinancial impact of denied claims by Specialty (total $ amount of denied claims/ total $ of claims for each specialty)\nWhile less than 5% is ideal, the industry average denial rate ranges from 5% to 10%. The goal for providers should be to settle 85% of denials in 30 days or less. (source: hfma.com)\n\nDenialRates &lt;- claimsdata %&gt;% \n  mutate(Year= year(ClaimDate), Month= month(ClaimDate)) %&gt;% \n  filter(Year == 2024) %&gt;% \n  group_by(ProviderSpecialty, Month) %&gt;% \n  summarize(\n    Total_ClaimAmount = sum(ClaimAmount, na.rm = TRUE),  ## Sum of all claims for each specialty\n    Denied_ClaimAmount = sum(ClaimAmount[ClaimStatus == \"Denied\"], na.rm = TRUE),\n    Percentage = round(Denied_ClaimAmount/ Total_ClaimAmount * 100,2)\n    ) ## Sum of denied claims for each specialty\n\n`summarise()` has grouped output by 'ProviderSpecialty'. You can override using\nthe `.groups` argument.\n\nview(DenialRates)\n\nLet‚Äôs visualize the DenialRates with a scatterplot for the most current year, 2024.\n\nggplot(DenialRates, aes(x= as.factor(Month), y= Percentage, color = ProviderSpecialty)) +\n  ggtitle(\"Percentage of Denied Claims by Provider Specialty 2024\") +\n  xlab(\"Month\") + ylab(\"Percentage\") +\n  geom_point(size = 3) +\n  scale_y_continuous(breaks = seq(0, 100, by = 10)) +  ## Y-axis increments by 1\n  scale_x_discrete(labels = c(\"1\" = \"Jan\", \"2\" = \"Feb\", \"3\" = \"Mar\", \"4\" = \"Apr\", \"5\" = \"May\", \"6\" = \"Jun\", \"7\" = \"Jul\", \"8\" = \"Aug\", \"9\" = \"Sep\", \"10\" = \"Oct\", \"11\" = \"Nov\", \"12\" = \"Dec\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWow! For the entire year of 2024, the General Practice specialty was the only specialty to meet the industry standard in July. Cardiology was consistently higher compared to the other specialties from April through July of 2024.\nI wonder what practices General practice was implementing for July of 2024, and how to implement them into the other specialties?\n\n\nLet‚Äôs keep going onto our third and final metric.\nTrends in claims submissions to see which are more successful. Cost per claim submission method (paper, online [electronic] or phone) and claim status (approved, denied and pending)\n95% is the industry standard benchmark- this implies that at least 95% of claims should be submitted electronically by healthcare providers. Because electronic claim filing is quicker, more accurate, and less expensive than paper-based submissions. (source: MD Clarity)\n\n## group data by provider specialty and claim submission methods \n\nclaims_status_by_method &lt;- claimsdata %&gt;%\n  filter(ClaimStatus== \"Approved\" | ClaimStatus== \"Denied\") %&gt;% \n  group_by(ClaimSubmissionMethod, ClaimStatus) %&gt;%\n  summarize(ClaimCount = n()) %&gt;%             ##  counts for each group.\n  ungroup() %&gt;%\n  group_by(ClaimSubmissionMethod) %&gt;%\n  mutate(Percentage = round(ClaimCount / sum(ClaimCount) * 100, 2)) %&gt;%\n  arrange(ClaimSubmissionMethod, -Percentage)  ## see which status is more prevalent for each submission method\n\n`summarise()` has grouped output by 'ClaimSubmissionMethod'. You can override\nusing the `.groups` argument.\n\n# View the calculated table\nprint(claims_status_by_method)\n\n# A tibble: 6 √ó 4\n# Groups:   ClaimSubmissionMethod [3]\n  ClaimSubmissionMethod ClaimStatus ClaimCount Percentage\n  &lt;chr&gt;                 &lt;chr&gt;            &lt;int&gt;      &lt;dbl&gt;\n1 Online                Denied             525       52.2\n2 Online                Approved           480       47.8\n3 Paper                 Approved           524       50.6\n4 Paper                 Denied             511       49.4\n5 Phone                 Approved           518       52.1\n6 Phone                 Denied             476       47.9\n\n\nThe percentages are calculated within each submission method (so for ‚ÄúOnline‚Äù, the total is 525 + 480 = 1005 claims, and the percentages are based on that total).\nThe analysis shows that even while online submissions are faster and less expensive, they still have a high rejection rate (52.2%).\nThe Paper method of submitting a claim has the highest chance of being approved.I wonder if there is a possibility that online submissions have stricter review processes, leading to more denials?"
  },
  {
    "objectID": "posts/ClaimsData/index.html#in-summary",
    "href": "posts/ClaimsData/index.html#in-summary",
    "title": "Better Care, Smarter Spending: Exploring Provider Performance and Claim Trends",
    "section": "In Summary",
    "text": "In Summary\nOrganizations can modify care models, promote evidence-based practices, and enhance patient outcomes by identifying inefficient or expensive care trends (such as overutilization in particular specialties).\nSpecific operational inefficiencies (such as high denial rates for electronic claims) can be identified with the use of metrics like denial rates and submission techniques. These inefficiencies can then be fixed by process optimization, improved tools, or training."
  }
]